// doc/tutorial_running.dox

// Copyright 2009-2011 Microsoft Corporation

// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at

//  http://www.apache.org/licenses/LICENSE-2.0

// THIS CODE IS PROVIDED *AS IS* BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
// KIND, EITHER EXPRESS OR IMPLIED, INCLUDING WITHOUT LIMITATION ANY IMPLIED
// WARRANTIES OR CONDITIONS OF TITLE, FITNESS FOR A PARTICULAR PURPOSE,
// MERCHANTABLITY OR NON-INFRINGEMENT.
// See the Apache 2 License for the specific language governing permissions and
// limitations under the License.

/**
 \page tutorial_running Running the example scripts (1 hour)

 - Up: \ref tutorial "Kaldi tutorial"
 - Previous: \ref tutorial_looking "Overview of the distribution"


\section tutorial_running_start Getting started, and prerequisites.

The next stage of the tutorial is to start running the example scripts for
Resource Management.  Change directory to the top level (we called it kaldi-1),
and then to egs/.  Look at the README.txt file in that directory, and
specifically look at the Resource Management section.  It mentions the LDC
catalog number corresponding to the corpus.  This may help you in obtaining the
data from the LDC.  If you cannot get the data for some reason, just continue
reading this tutorial and doing the steps that you can do without the data, and
you may still obtain some value from it.  The best case is that there is some
directory on your system, say /mnt/data4/RM, that contains three subdirectories;
call them rm1_audio1, rm1_audio2 and rm2_audio.  These would correspond to the
three original disks in the data distribution from the LDC, with no changes in
format.  These instructions assume your shell is bash.  If you have a different
shell, these commands will not work or should be modified (just type "bash"
to get into bash, and everything should work).


Now change directory to rm/, glance at the file README.txt to see what the
overall structure is, and cd to s1/.  This is the basic sequence of experiments
that corresponds to the main functionality in version 1 of the toolkit.

In s1/, list the directory and glance at the RESULTS file so you have some
idea what is in there (later on, you should verify that the results you get
are similar to what is in there).  The main file we will be looking at
is run.sh.  Note: run.sh is not intended to be run directly from the shell;
the idea is that you run the commands in it one by one, by hand.

\section tutorial_running_data_prep Data preparation

The first three lines you have to run are as follows [assuming your data is
in /mnt/data4/RM, which is an example]:
\verbatim
cd data_prep
./run.sh /mnt/data4/RM
cd ..
\endverbatim
If this works it should say "Succeeded".  If not, you will have to work out
where the script failed and what the problem was.

The next couple of steps in run.sh copy some things from data_prep/ into data/.
Run these.  The general concept behind this directory structure is that
data_prep/ contains the corpus-specific data preparation stages and data/
contains the same data in some kind of "normalized" form, so that in theory the
same set of system-building works could run from the same data/ directory.
However, it is not perfectly executed and we will improve the organization of the
scripts at some point.

At this point let's have a quick look at the things that were prepared
in data_prep/.  Change directory to data_prep/, and run the following
commands, looking at the output each time.
\verbatim
 # G.txt is the word-pair grammar supplied with the RM corpus.
 head G.txt   
 head lexicon.txt
 head train_trans.txt
 head train_wav.scp
\endverbatim
These will give you some idea of what the outputs of a generic data preparation process would
look like.  Something you should appreciate is that not all of these are "native" Kaldi
formats:
 - The grammar, G.txt, needs to be compiled into an FST before it is used, and even then
   it is not read directly by Kaldi but is processed further with OpenFst tools before
   being used.
 - The lexicon, lexicon.txt, needs to be converted by OpenFst into the binary FST format
   before Kaldi will read it (and we'll turn the words and phones into integer labels;
   Kaldi will only deal with integers).
 - The transcriptions, train_trans.txt, will also be turned into an integer format
   (but still text),  with the words replaced with integers.
 - The file train_wav.scp is actually read directly by Kaldi programs.  Look at the file
   again.  It is parsed as a set of key-value pairs, where the key is the first string on
   each line.  The value is a kind of "extended filename", and you can guess how it works.  
   Since it is for reading we will refer to this type of string as an "rxfilename" (for writing
   we use the term wxfilename).  See \ref io_sec_xfilename if you are curious.  Note that
   although we use the extension .scp, this is not a script file in the HTK sense (i.e. it
   is not viewed as an extension to the command-line arguments). 

Now, from the directory s1/, run the next step in run.sh which is:
\verbatim
 steps/prepare_graphs.sh
\endverbatim
Look at the script.  It transforms some of the files created in data_prep/ to a more
normalized form that is read by Kaldi.  This script creates its output in the
data/ directory.  The files we mention below will be in that directory.

The first two files this script creates are called words.txt and phones.txt.
These are OpenFst format symbol tables, and represent a mapping from strings to
integers and back.

Look at the files with suffix .csl.  These are colon-separated lists of
the integer id's of non-silence, and silence, phones respectively.  They are sometimes
needed as options on program command lines (e.g. to specify lists of silence phones).

Look at phones_disambig.txt.  This file is a phone symbol table that also 
handles the "disambiguation symbols" used in the standard FST recipe.
These symbols are conventionally called \#1, \#2 and so on;
 see the paper <a href=www.cs.nyu.edu/~mohri/pub/hbka.pdf> "Speech Recognition
with Weighted Finite State Transducers" </a>.  We also add a symbol \#0
which replaces epsilon transitions in the language model; see
\ref graph_disambig for more information.

The file L.fst is the compiled lexicon in FST format.  To see what kind of information
is in it, you can (from s1/), do:
\verbatim
 . path.sh
 fstprint --isymbols=data/phones_disambig.txt --osymbols=data/words.txt data/L.fst | head
\endverbatim
The FST format is not very human-readable.  Look at the command-line that creates L.fst
in steps/prepare_graphs.sh.  Try to figure out what the 0.5 on the command line means
(you will have to look at the Perl script that is invoked).

\section tutorial_running_feats Feature extraction

The next step is to extract the training features.  Search for "mfcc" in run.sh and
run the corresponding three lines of script (you have to decide where you want to put the 
features first and modify the example accordingly).  Suppose we decide to put the
features on /my/disk/rm_mfccdir, we would do something like:
\verbatim
mkdir /my/disk/rm_mfccdir
steps/make_mfcc_train.sh /my/disk/rm_mfccdir
steps/make_mfcc_test.sh /my/disk/rm_mfccdir
\endverbatim
Run these jobs.  They
use several CPUs in parallel and should be done in around two minutes on a fast
machine.  

In the script steps/make_mfcc_train.sh,
look at the line that invokes split_scp.pl.  By doing a word count of data_prep/train_wav.scp
and the files matched by the pattern exp/make_mfcc/train_wav?.scp, you can see what this
line does.
Next look at the line that invokes compute-mfcc-feats.  The options should be fairly self-explanatory.
The option that involves the config file is a mechanism that can be used in Kaldi to pass configuration
options, like a HTK config file, but it is quite rarely used.  The positional arguments (the
ones that begin with "scp" and "ark,scp" require a little more explanation.  

Before we explain this, have a look at the command line in the script again and examine
the inputs and outputs using:
\verbatim
head exp/make_mfcc/train_wav1.scp
head /my/disk/rm_mfccdir/train_raw_mfcc1.scp
less /my/disk/rm_mfccdir/train_raw_mfcc1.ark
\endverbatim
(in these commands you would replace /my/disk/rm_mfccdir/ with the actual directory.
Be careful-- the .ark file contains binary data (you may have to type "reset" if your terminal
doesn't work right after viewing this data).

By listing the files you can see that the .ark files are quite big (because they contain
the actual data).  You can view one of these archive files more conveniently by typing:
\verbatim
. path.sh
copy-feats ark:/my/disk/rm_mfccdir/train_raw_mfcc1.ark ark,t:- | head
\endverbatim
[From now we will omit the ". path.sh" command and assume you have already done it].
You can remove the ",t" modifier from this command and try it again if you like-- but
it might be a good to pipe it into "less" because the data will be binary.
An alternative way to view the same data is to do:
\verbatim
copy-feats scp:data/train.scp ark,t:- | head
\endverbatim
This is because the archive and script file both represent the same data.  Notice
the "scp:" and "ark:" prefixes in these commands.  Kaldi doesn't attempt to work
out whether something is a script file or archive format from the data itself,
and in fact Kaldi never attempts to work things out from file suffixes.  This is
for general philosophical reasons, and also to forestall bad interaction with
pipes (because pipes don't have a filename).

Now type the following command:
\verbatim
head -10 data/train.scp | tail -1 | copy-feats scp:- ark,t:- | head
\endverbatim
This prints out some data from the tenth training file.  Notice that in
"scp:-", the "-" tells it to read from the standard input, while "scp" tells
it to interpret the input as a script file.

Next we will describe what script and archive files actually are.
The first point we want to make is that the code sees both of them
in the same way.  For a particularly simple example of the user-level
calling code, type the following command:
\verbatim
tail -30 ../../../src/featbin/copy-feats.cc
\endverbatim

The basic concept is the concept of an ordered set of items (e.g. feature files),
indexed by strings (e.g. utterance identifiers).  We call this a Table (it is not
really a C++ object, as we have separate C++ objects to access the data depending
whether we are writing, iterating, or doing random access).  The .scp format has
lines with a key, and then an "extended filename" that tells Kaldi where to find
the data.  The archive format may be text or binary (you can write in text mode
with the ",t" modifier).  The format is: the key (e.g. utterance id), then a
space, then the object data.  Kaldi doesn't attempt to put the object type into
the archive; you have to know the object type in advance (archives can't contain
mixtures of types).  Many times, we will pipe data between programs using
archives on the standard input and output; when this happens, you will see the
string "ark:-" as one of the command line arguments.  For more details on this
topic you can see \ref io.

\section tutorial_running_feats Monophone training

The next step is to train monophone models.  If the disk where you installed
Kaldi is not big, you might want to make exp/ a soft link to a directory somewhere
on a big disk (if you run all the experiments and don't clean up, it can get up 
to a few gigabytes).  Type
\verbatim
nohup steps/train_mono.sh &
\endverbatim
You can view the most recent output of this by typing
\verbatim
tail nohup.out
\endverbatim
(we run longer jobs this way so they can finish running even if we get disconnected).
There is actually very little output that goes to the standard out and error of this
script; most of it goes to log files in exp/mono/.

While it is running, look at the file exp/mono/topo.  This file is created immediately.
One of the phones has a different topology from the others.  Look at data/phones.txt
in order to figure out from the numeric id which phone it is.  Notice that each entry in
the topology file has a final state with no transitions out of it.  The convention in
the topology files is that the first state is initial (with probability one) and the
last state is final (with probability one).

Type 
\verbatim
less exp/mono/0.mdl
\endverbatim
and look at the model file.  You will see that it contains the information in 
topology file at the top of it, and then some other things, before the model parameters.
The convention is that the .mdl file contains two objects: one of type TransitionModel,
and one of the relevant model type (in this case, type AmGmm).  By "contains two objects",
what we mean is that the objects have Write and Read functions in a standard form, and
we call these functions to write the objects to the file.  For objects such as this,
that are not part of a Table (i.e. there is no "ark:" or "scp:" involved), writing is
in binary or text mode and can be controlled by the standard command-line 
options --binary=true or --binary=false (different programs have different defaults).
Glance through the model file to see what kind of information it contains.  At this
point we won't go into more detail on how models are represented in Kaldi; see
hmm to find out more.  

We will mention one important point, though: p.d.f.'s in Kaldi are represented by
numeric id's, starting from zero (we call these pdf-ids).  They do not have
"names", as in HTK.  The .mdl file does not have sufficient information to map
between context-dependent phones and pdf-ids.  For the tree file, look at
exp/mono/tree.  Note that this is a monophone "tree" so it is very trivial-- it
does not have any "splits".  Although this tree format was not indended to be
very human-readable, because we have received a number of queries about this, we
will explain it.  The rest of this paragraph can be skipped over by the casual reader.
After "ToPdf", the tree file contains an object of the
polymorphic type EventMap, which can be thought of as storing a mapping from a
set of integer (key,value) pairs representing the phone-in-context and HMM state,
to a numeric p.d.f. id.  Derived from EventMap are the types ConstantEventMap
(representing the leaves of the tree), TableEventMap (representing some kind of
lookup table) and SplitEventMap (representing a tree split).  In this file
exp/mono/tree, "CE" is a marker for ConstantEventMap (and corresponds to the
leaves of the tree), and "TE" is a marker for TableEventMap (there is no "SE", or
SplitEventMap, because this is the monophone case).  "TE 0 49" is the start of a
TableEventMap that "splits" on key zero (representing the zeroth phone position
in a phone-context vector of length one, for the monophone case).  It is
followed, in parentheses by 49 objects of type EventMap.  The first one is NULL
(representing a zero pointer to EventMap) because the phone-id zero is reserved
for "epsilon").  An example non-NULL object is the string "TE -1 3 ( CE 33 CE 34
CE 35 )", which represents a TableEventMap splitting on key -1.  The value
assigned to this key can take the values 0, 1 or 2 for this phone, and
corresponds to the numeric PdfClass specified in the topology file.   In our
topology file this is identical to the HMM-state index.  Inside the parentheses are three
objects of type ConstantEventMap, each representing a leaf of the tree.

Now look at the file exp/mono/cur.ali (it should exist if the training has progressed
far enough).  This is the Viterbi alignment of the training data; it has one line
for each training file.  Now do "tail exp/mono/tree" and look for the highest-numbered
p.d.f. id (which is the last number in the file).  Compare this with the numbers in
exp/mono/cur.ali.  Does something seem wrong?  The reason is that the alignment file
does not contain p.d.f. id's.  It contains a slightly more fine-grained identifier
that we call a "transition-id".  This also encodes the phone, and the transition within
the prototype topology of the phone.  This is useful for a number of reasons.
For more details, see \ref hmm.

Next let's look at how training is progressing (this step assumes your shell is bash).
Type
\verbatim
grep Overall exp/mono/acc.{?,??}.log
\endverbatim
You can see the acoustic likelihods on each iteration.  Next look at one of the files
exp/mono/update.*.log to see what kind of information is in the update log.

\section tutorial_looking_whats What's in the code

Now that we have a vague idea of what the code looks like, where it is, and how it is
compiled, we are going to briefly skim over some parts of the code to give you an idea
what is in there.  

First look at the file base/kaldi-common.h.  This file includes a number of
things from the base/ directory that are used by almost every Kaldi program.  You
can mostly guess from the filenames the types of things that are provided: things
like error-logging macros, typedefs, math utility functions such as random number
generation, and miscellaneous \#defines.  But this is a stripped-down set of
utilities; look at util/common-utils.h to see a more complete set, including
command-line parsing and I/O functions that handle extended filenames such as
pipes.  Some of the other I/O related utilities included in util/common-utils.h
are hard to describe concisely, and we'll come to them in time.  The reason why
we segregated a subset of utilities into the base/ directory is so that we could
minimize the set of things that the matrix/ library depends on (since it's useful
independent of the rest of Kaldi).

Look at the file matrix/matrix-lib.h.  See what files it includes.  This provides
an overview of the kinds of things that are in the matrix library.  This library
is basically a C++ wrapper for BLAS and LAPACK, in case that means anything to you.
The files sp-matrix.h and tp-matrix relate to symmetric packed matrices and
triangular packed matrices, respectively.  Quickly scan the file matrix/kaldi-matrix.h.
This will give you some idea what the matrix code looks like.  It consists of
a C++ class representing a matrix.  We provide a mini-tutorial on the matrix
library \ref matrix "here", if you are interested.  Scanning through
matrix/matrix-lib-test.cc will give you some idea how the various matrix
and vector functions are called.

Next look at gmm/diag-gmm.h (this class stores a Gaussian Mixture Model).  
The class DiagGmm may look a bit confusing as
it has many different accessor functions.  Search for "private" and look
at the class member variables (they always end with an underscore, as per
the Kaldi style).  This should make it clear how we store the GMM.
This is just a single GMM, not a whole collection of GMMs. 
Look at gmm/am-diag-gmm.h; this class stores a collection of GMMs.
Notice that it does not inherit from anything.
Search for "private" and you can see the member variables (there
are only two of them).  You can understand from this how simple the
class is (everything else consists of various accessors and convenience
functions).  A natural question to ask is: where are the transitions,
where is the decision tree, and where is the HMM topology?  All of these
things are kept separate from the acoustic model, because it's likely
that researchers might want to replace the acoustic likelihoods while
keeping the rest of the system the same.  We'll come to all this stuff later.

Next look at feat/feature-mfcc.h.  Focus on the MfccOptions struct.
The struct members give you some idea what kind of options are supported
in MFCC feature extraction.  
Notice that some struct members are options structs themselves.
Look at the Register function.  This is standard in Kaldi options classes.
Then look at featbin/compute-mfcc-feats.cc and search for Register.
You can see where this function is called from the command-line program.
To see a complete list of the options supported for MFCC feature extraction,
execute the program featbin/compute-mfcc-feats with no arguments.
Recall that you saw some of these options being registered in 
the MfccOptions class, and others being registered in 
featbin/compute-mfcc-feats.cc.  The way to specify options is --option=value.

Next look at tree/build-tree.h.  Find the BuildTree function.  This is the main
top-level function for building the decision tree.  Notice that it returns a
pointer the type EventMap.  This is a type that stores a function from a set of
(key, value) pairs to an integer.  It's defined in tree/event-map.h.  The keys
and values are both integers, but the keys represent phonetic-context positions
(typically 0, 1 or 2) and the values represent phones.  There is also a special
key, -1, that roughly represents the position in the HMM.  There are a lot of
details inside the tree-building code: list all the header files in the tree
directory.  For now we won't discuss it further.

Next look at hmm/hmm-topology.h.  The class HmmTopology defines a set of HMM
topologies for a number of phones.  In general each phone can have a different
topology.  The topology includes "default" transitions, used for initialization.
Look at the example topology in the extended comment at the top of the header.
There is a tag <PdfClass> (note: this format is vaguely XML-like, but it is not
really XML).  The <PdfClass> is always the same as the HMM-state (<State>).  In
general it doesn't have to be.  This is a mechanism to enforce tying of
distributions between distinct HMM states; it's possibly useful if you want to
create more interesting transition models.

  - Up: \ref tutorial "Kaldi tutorial"
  - Previous: \ref tutorial_looking "Overview of the distribution"

*/
