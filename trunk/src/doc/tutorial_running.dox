// doc/tutorial_running.dox

// Copyright 2009-2011 Microsoft Corporation

// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at

//  http://www.apache.org/licenses/LICENSE-2.0

// THIS CODE IS PROVIDED *AS IS* BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
// KIND, EITHER EXPRESS OR IMPLIED, INCLUDING WITHOUT LIMITATION ANY IMPLIED
// WARRANTIES OR CONDITIONS OF TITLE, FITNESS FOR A PARTICULAR PURPOSE,
// MERCHANTABLITY OR NON-INFRINGEMENT.
// See the Apache 2 License for the specific language governing permissions and
// limitations under the License.

/**
 \page tutorial_running Running the example scripts (40 minutes)

  \ref tutorial "Up: Kaldi tutorial" <BR>
  \ref tutorial_looking "Previous: Overview of the distribution" <BR>
  \ref tutorial_code "Next: Taking a look at the code" <BR>


\section tutorial_running_start Getting started, and prerequisites.

The next stage of the tutorial is to start running the example scripts for
Resource Management.  Change directory to the top level (we called it kaldi-1),
and then to egs/.  Look at the README.txt file in that directory, and
specifically look at the Resource Management section.  It mentions the LDC
catalog number corresponding to the corpus.  This may help you in obtaining the
data from the LDC.  If you cannot get the data for some reason, just continue
reading this tutorial and doing the steps that you can do without the data, and
you may still obtain some value from it.  The best case is that there is some
directory on your system, say /mnt/data4/RM, that contains three subdirectories;
call them rm1_audio1, rm1_audio2 and rm2_audio.  These would correspond to the
three original disks in the data distribution from the LDC.
These instructions assume that your shell is bash.  If you have a different
shell, these commands will not work or should be modified (just type "bash"
to get into bash, and everything should work).

Now change directory to rm/, glance at the file README.txt to see what the
overall structure is, and cd to s1/.  This is the basic sequence of experiments
that corresponds to the main functionality in version 1 of the toolkit.

In s1/, list the directory and glance at the RESULTS file so you have some
idea what is in there (later on, you should verify that the results you get
are similar to what is in there).  The main file we will be looking at
is run.sh.  Note: run.sh is not intended to be run directly from the shell;
the idea is that you run the commands in it one by one, by hand.

\section tutorial_running_data_prep Data preparation

The first three lines you have to run are as follows [assuming your data is
oin /mnt/data4/RM, which is an example]:
\verbatim
cd data_prep
./run.sh /mnt/data4/RM
cd ..
\endverbatim
If this works it should say "Succeeded".  If not, you will have to work out
where the script failed and what the problem was.

The next couple of steps in s1/run.sh copy some things from data_prep/ into data/.
Run these.  The general concept behind this directory structure is that
data_prep/ contains the corpus-specific data preparation stages and data/
contains the same data in some kind of "normalized" form, so that in theory the
same set of system-building works could run from the same data/ directory.
However, it is not perfectly executed and we will improve the organization of the
scripts at some point.

At this point let's have a quick look at the things that were prepared
in data_prep/.  Change directory to data_prep/, and run the following
commands, looking at the output each time.
\verbatim
 # G.txt is the word-pair grammar supplied with the RM corpus.
 head G.txt   
 head lexicon.txt
 head train_trans.txt
 head train_wav.scp
\endverbatim
These will give you some idea of what the outputs of a generic data preparation process would
look like.  Something you should appreciate is that not all of these files are "native" Kaldi
formats, i.e. not all of them could be read by Kaldi's C++ programs:
 - The grammar, G.txt, needs to be compiled into an FST before it is used, and even then
   it is not generally read directly by Kaldi but would be processed further using OpenFst tools.
 - The lexicon, lexicon.txt, needs to be converted by OpenFst into the binary FST format
   before Kaldi will read it (and we'll turn the words and phones into integer labels;
   Kaldi will only deal with integers).
 - The transcriptions, train_trans.txt, will also be turned into an integer format--
   still a text file, but with the words replaced with integers.
 - The file train_wav.scp is actually read directly by Kaldi programs when doing feature 
   extraction.  Look at the file
   again.  It is parsed as a set of key-value pairs, where the key is the first string on
   each line.  The value is a kind of "extended filename", and you can guess how it works.  
   Since it is for reading we will refer to this type of string as an "rxfilename" (for writing
   we use the term wxfilename).  See \ref io_sec_xfilename if you are curious.  Note that
   although we use the extension .scp, this is not a script file in the HTK sense (i.e. it
   is not viewed as an extension to the command-line arguments). 

Now, from the directory s1/, run the next step in run.sh which is:
\verbatim
 steps/prepare_graphs.sh
\endverbatim
Look at the script.  It transforms some of the files created in data_prep/ to a more
normalized form that is read by Kaldi.  This script creates its output in the
data/ directory.  The files we mention below will be in that directory.

The first two files this script creates are called words.txt and phones.txt
(both in the directory data/).
These are OpenFst format symbol tables, and represent a mapping from strings to
integers and back.  Look at these files; since they are important and will be
frequently used so you need to understand what is in them.  They have the same
format as the symbol table format we encountered previously in \ref tutorial_looking
"Overview of the distribution".

Look at the files with suffix .csl (in data/).  These are colon-separated lists of
the integer id's of non-silence, and silence, phones respectively.  They are sometimes
needed as options on program command lines (e.g. to specify lists of silence phones),
and for other purposes.

Look at phones_disambig.txt.  This file is a phone symbol table that also 
handles the "disambiguation symbols" used in the standard FST recipe.
These symbols are conventionally called \#1, \#2 and so on;
 see the paper <a href=www.cs.nyu.edu/~mohri/pub/hbka.pdf> "Speech Recognition
with Weighted Finite State Transducers" </a>.  We also add a symbol \#0
which replaces epsilon transitions in the language model; see
\ref graph_disambig for more information.  How many disambiguation symbols
are there?  In some recipes the number of disambiguation symbols is the same
as the maximum number of words that share the same pronunciation.  In our recipe
there are a few more; you can find more explanation \ref graph_disambig "here".

The file L.fst is the compiled lexicon in FST format.  To see what kind of information
is in it, you can (from s1/), do:
\verbatim
 . path.sh
 fstprint --isymbols=data/phones_disambig.txt --osymbols=data/words.txt data/L.fst | head
\endverbatim
Look at the command-line that creates L.fst
in steps/prepare_graphs.sh.  Try to figure out what the 0.5 on the command line means
(you will have to look at the Perl script that is invoked).

\section tutorial_running_feats Feature extraction

The next step is to extract the training features.  Search for "mfcc" in run.sh and
run the corresponding three lines of script (you have to decide where you want to put the 
features first and modify the example accordingly).  Suppose we decide to put the
features on /my/disk/rm_mfccdir, we would do something like:
\verbatim
mkdir /my/disk/rm_mfccdir
steps/make_mfcc_train.sh /my/disk/rm_mfccdir
steps/make_mfcc_test.sh /my/disk/rm_mfccdir
\endverbatim
Run these jobs.  They
use several CPUs in parallel and should be done in around two minutes on a fast
machine.  Look at the file exp/make_mfcc/make_mfcc_train.1.log to see the logging
output of the program that creates the MFCCs.  At the top of it you will see the
command line (Kaldi programs always echo the command line unless you specify
--print-args=false).  

In the script steps/make_mfcc_train.sh,
look at the line that invokes split_scp.pl.  You can probably guess what this does.
Type

By typing
\verbatim
wc data_prep/train_wav.scp
wc exp/make_mfcc/train_wav?.scp
\endverbatim
you can confirm it.
Next look at the line that invokes compute-mfcc-feats.  The options should be
fairly self-explanatory.  The option that involves the config file is a
mechanism that can be used in Kaldi to pass configuration options, like a HTK
config file, but it is actually quite rarely used.  The positional arguments
(the ones that begin with "scp" and "ark,scp" require a little more explanation.

Before we explain this, have a look at the command line in the script again and examine
the inputs and outputs using:
\verbatim
head exp/make_mfcc/train_wav1.scp
head /my/disk/rm_mfccdir/train_raw_mfcc1.scp
less /my/disk/rm_mfccdir/train_raw_mfcc1.ark
\endverbatim
(in these commands you would replace /my/disk/rm_mfccdir/ with the actual directory where
you put your mfcc data.
Be careful-- the .ark file contains binary data (you may have to type "reset" if your terminal
doesn't work right after looking at it).

By listing the files you can see that the .ark files are quite big (because they contain
the actual data).  You can view one of these archive files more conveniently by typing:
\verbatim
. path.sh
copy-feats ark:/my/disk/rm_mfccdir/train_raw_mfcc1.ark ark,t:- | head
\endverbatim
[From now we will omit the ". path.sh" command and assume you have already done it].
You can remove the ",t" modifier from this command and try it again if you like-- but
it might be a good to pipe it into "less" because the data will be binary.
An alternative way to view the same data is to do:
\verbatim
copy-feats scp:data/train.scp ark,t:- | head
\endverbatim
This is because these archive and script files both represent the same data (well, technically
the archive only represents one quarter of it because we split it into four pieces).  Notice
the "scp:" and "ark:" prefixes in these commands.  Kaldi doesn't attempt to work
out whether something is a script file or archive format from the data itself,
and in fact Kaldi never attempts to work things out from file suffixes.  This is
for general philosophical reasons, and also to forestall bad interaction with
pipes (because pipes don't normally have a name).

Now type the following command:
\verbatim
head -10 data/train.scp | tail -1 | copy-feats scp:- ark,t:- | head
\endverbatim
(remember to source path.sh by entering ". path.sh" if it fails).
This prints out some data from the tenth training file.  Notice that in
"scp:-", the "-" tells it to read from the standard input, while "scp" tells
it to interpret the input as a script file.

Next we will describe what script and archive files actually are.
The first point we want to make is that the code sees both of them
in the same way.  For a particularly simple example of the user-level
calling code, type the following command:
\verbatim
tail -30 ../../../src/featbin/copy-feats.cc
\endverbatim
You can see that the part of this program that actually does the work is just
three lines of code (actually there are two branches, each with three lines
of code).  If you are familiar with the StateIterator type in OpenFst you will 
notice that the way we iterate is in the same style (we have tried to be
as style-compatible as OpenFst as possible).

Underlying scripts and archives is the concept of a Table.  A Table is basically
an ordered set of items (e.g. feature files), indexed by unique strings
(e.g. utterance identifiers).  A Table is not really a C++ object, because we have
separate C++ objects to access the data depending whether we are writing,
iterating, or doing random access.  An example of these types where the object
in question is a matrix of floats (Matrix<BaseFloat>), is:
\verbatim
BaseFloatMatrixWriter
RandomAccessBaseFloatMatrixReader
SequentialBaseFloatMatrixReader
\endverbatim
These types are all typedefs that are actually templated classes.  We won't go
into further detail here.
A script (.scp) file or an archive (.ark) file
are both viewed as Tables of data.  The formats are as follows:

 - The .scp format is a text-only format has lines with a key, and then an "extended filename" 
   that tells Kaldi where to find the data.  
 - The archive format may be text or binary (you can write in text mode
   with the ",t" modifier; binary is default).  The format is: the key (e.g. utterance id), then a
   space, then the object data.  

A few generic points about scripts and archives:
 - A string that specifies how to read a Table (archive or script) is called an rspecifier;
   for example "ark:gunzip -c my/dir/foo.ark.gz|".
 - A string that specifies how to write a Table (archive or script) is called a wspecifier;
   for example "ark,t:foo.ark".
 - Archives can be concatenated together and still be valid archives (there is no
  "central index" in them). 
 - The code can read both scripts and archives either sequentially or via random access.
   The user-level code only knows whether it's iterating or doing lookup; it doesn't
   know whether it's accessing a script or an archive.
 - Kaldi doesn't attempt to represent the object type in
   the archive; you have to know the object type in advance 
 - Archives and script files can't contain mixtures of types. 
 - Reading archives via random access can be memory-inefficient as the code may have
   to cache the objects in memory.
 - For efficient random access to an archive, you can write out a corresponding 
   script file using the "ark,scp" writing mechanism (e.g., used in writing the mfcc
   features to disk).  You would then access it via the scp file.
 - Another way to avoid the code having to cache a bunch of stuff in memory when doing
   random access on archives is
   to tell the code that the archive is sorted and will be called in sorted 
   order (e.g. "ark,s,cs:-").
 - Types that read and write archives are templated on a Holder type, which is a type
   that "knows how" to read and write the object in question.

 Here we have just given a very quick overview that will probably raise more questions
 than it provides answers; it is just intended to make you aware of the kinds of
 issues involved.  For more details, see \ref io.

To give you some idea how archives and script files can be used within pipes,
type the following command and try to understand what is going on:
\verbatim
head -1 data/train.scp | copy-feats scp:- ark:- | copy-feats ark:- ark,t:- | head
\endverbatim

\section tutorial_running_monophone Monophone training

The next step is to train monophone models.  If the disk where you installed
Kaldi is not big, you might want to make exp/ a soft link to a directory somewhere
on a big disk (if you run all the experiments and don't clean up, it can get up 
to a few gigabytes).  Type
\verbatim
nohup steps/train_mono.sh &
\endverbatim
You can view the most recent output of this by typing
\verbatim
tail nohup.out
\endverbatim
(we run longer jobs this way so they can finish running even if we get disconnected).
There is actually very little output that goes to the standard out and error of this
script; most of it goes to log files in exp/mono/.

While it is running, look at the file exp/mono/topo.  This file is created immediately.
One of the phones has a different topology from the others.  Look at data/phones.txt
in order to figure out from the numeric id which phone it is.  Notice that each entry in
the topology file has a final state with no transitions out of it.  The convention in
the topology files is that the first state is initial (with probability one) and the
last state is final (with probability one).

Type 
\verbatim
less exp/mono/0.mdl
\endverbatim
and look at the model file.  You will see that it contains the information in 
topology file at the top of it, and then some other things, before the model parameters.
The convention is that the .mdl file contains two objects: one object of type TransitionModel,
which contains the topology information as a member variable of type HmmTopology, 
and one object of the relevant model type (in this case, type AmGmm).  
By "contains two objects", what we mean is that the objects have Write and Read
functions in a standard form, and we call these functions to write the objects
to the file.  For objects such as this, that are not part of a Table (i.e. there
is no "ark:" or "scp:" involved), writing is in binary or text mode and can be
controlled by the standard command-line options --binary=true or --binary=false
(different programs have different defaults).  For Tables (i.e. archives and scripts),
binary or text model is controlled by the ",t" option in the specifier.

Glance through the model file to see what kind of information it contains.  At
this point we won't go into more detail on how models are represented in Kaldi;
see \ref hmm to find out more.

We will mention one important point, though: p.d.f.'s in Kaldi are represented by
numeric id's, starting from zero (we call these pdf-ids).  They do not have
"names", as in HTK.  The .mdl file does not have sufficient information to map
between context-dependent phones and pdf-ids.  For that information, see the tree file:
do
\verbatim
less exp/mono/tree
\endverbatim
Note that this is a monophone "tree" so it is very trivial-- it
does not have any "splits".  Although this tree format was not indended to be
very human-readable, we have received a number of queries about the tree format so we
will explain it.  The rest of this paragraph can be skipped over by the casual reader.
After "ToPdf", the tree file contains an object of the
polymorphic type EventMap, which can be thought of as storing a mapping from a
set of integer (key,value) pairs representing the phone-in-context and HMM state,
to a numeric p.d.f. id.  Derived from EventMap are the types ConstantEventMap
(representing the leaves of the tree), TableEventMap (representing some kind of
lookup table) and SplitEventMap (representing a tree split).  In this file
exp/mono/tree, "CE" is a marker for ConstantEventMap (and corresponds to the
leaves of the tree), and "TE" is a marker for TableEventMap (there is no "SE", or
SplitEventMap, because this is the monophone case).  "TE 0 49" is the start of a
TableEventMap that "splits" on key zero (representing the zeroth phone position
in a phone-context vector of length one, for the monophone case).  It is
followed, in parentheses, by 49 objects of type EventMap.  The first one is NULL,
representing a zero pointer to EventMap, because the phone-id zero is reserved
for "epsilon".  An example non-NULL object is the string "TE -1 3 ( CE 33 CE 34
CE 35 )", which represents a TableEventMap splitting on key -1.  This key represents
the PdfClass specified in the topology file, which in our example
is identical to the HMM-state index.  This phone has 3 HMM states, so the value
assigned to this key can take the values 0, 1 or 2.
Inside the parentheses are three objects of type ConstantEventMap, each representing 
a leaf of the tree.

Now look at the file exp/mono/cur.ali (it should exist if the training has progressed
far enough).  This is the Viterbi alignment of the training data; it has one line
for each training file.  Now do "tail exp/mono/tree" and look for the highest-numbered
p.d.f. id (which is the last number in the file).  Compare this with the numbers in
exp/mono/cur.ali.  Does something seem wrong?  The reason is that the alignment file
does not contain p.d.f. id's.  It contains a slightly more fine-grained identifier
that we call a "transition-id".  This also encodes the phone and the transition within
the prototype topology of the phone.  This is useful for a number of reasons.
For more details, see \ref hmm.

Next let's look at how training is progressing (this step assumes your shell is bash).
Type
\verbatim
grep Overall exp/mono/acc.{?,??}.log
\endverbatim
You can see the acoustic likelihods on each iteration.  Next look at one of the files
exp/mono/update.*.log to see what kind of information is in the update log.

When the monophone training is finished, start the monophone decoding with:
\verbatim
steps/decode_mono.sh &
\endverbatim
It first puts some output on the screen that comes from the graph creation
process.  Look at the script steps/decode_mono.sh and see where it invokes
the script to create the graph (near the top).  Then view the graph-creation
script that is invoked (scripts/mkgraph.sh).  Look at the programs that this 
script calls.  The names of many of them start with "fst" (e.g. fsttablecompose),
most of these programs are not actually from the OpenFst distribution.  We created
some of our own FST-manipulating programs.  You can find out
where these programs are located as follows.  Take an arbitrary program that
is invoked in scripts/mkgraph.sh (say, fstdeterminizestar).  Then type:
\verbatim
. path.sh
which fstdeterminizestar
\endverbatim
The reason why we have different versions of the programs is mostly because we
have a slightly different (less AT&T-ish) way of using FSTs in speech recognition.
For example, "fstdeterminizestar" corresponds to "classical" determinization in which
we remove epsilon arcs.  See \ref graph for more information.
To see some of the decoded output
\verbatim
less exp/decode_mono/decode_mar87.log 
\endverbatim
You can see that it puts the transcript on the screen.  The text form of the
transcript only appears in the logging information: the actual output of this
program appears in the file exp/decode_mono/test_mar87.tra.  Look at this file.
Then view it as words by typing:
\verbatim
scripts/int2sym.pl --ignore-first-field data/words.txt exp/decode_mono/test_mar87.tra 
\endverbatim
There is a corresponding script called sym2int.pl.  You can convert it back
to integer form by typing:
\verbatim
scripts/int2sym.pl --ignore-first-field data/words.txt exp/decode_mono/test_mar87.tra | \
 scripts/sym2int.pl --ignore-first-field data/words.txt 
\endverbatim
The \verb|--ignore-first-field| option is so that it doesn't try to convert the utterance
id to an integer.
Next, try doing
\verbatim
tail exp/decode_mono/decode_mar87.log 
\endverbatim
It will print out some useful summary information at the end, including the
real-time factor and the average log-likelihood per frame.  The real-time factor
will typically be about 0.2 to 0.3 (i.e. faster than real time).  This depends
on your CPU, how many jobs were on the machine and other factors.  This script
runs six jobs in parallel, so if your machine has fewer than six cores it may be
much slower.  Note that we use a fairly wide beam (20), for accurate results; in a
typical LVCSR setup, the beam would be much smaller (e.g. around 13).

Look at the top of the log file again, and focus on the command line.  The optional
arguments are before the positional arguments (this is mandatory).  In a separate
window (in the directory s1/), type
\verbatim
. path.sh
gmm-decode-faster
\endverbatim
to see the usage message, and match up the arguments with what you see in the log file.
Recall that "rspecifier" is one of those strings that specifies how to read a table,
and "wspecifier" specifies how to write one.  Look carefuly at these arguments and try
to figure out what they mean.  Look at the rspecifier that corresponds to the features, and
try to understand it (this one has spaces inside, so Kaldi prints it out with single quotes
around it so that you could paste it into the shell and the program would run as intended).

Next, type
\verbatim
nohup steps/train_tri1.sh &
\endverbatim
and while this is running (it should take about ten minutes), we will do the next
step of tutorial.

  \ref tutorial "Up: Kaldi tutorial" <BR>
  \ref tutorial_looking "Previous: Overview of the distribution" <BR>
  \ref tutorial_code "Next: Taking a look at the code" <BR>
<P>

*/
