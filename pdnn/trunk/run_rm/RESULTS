#!/bin/bash

# results of tri3b, the SAT system
# for unknown reasons, I was unable to reproduce the number released with kaldi, although I used exactly
# the same script and data release. 
# %WER 1.90 [ 238 / 12533, 26 ins, 50 del, 162 sub ] exp/tri3b/decode/wer_4

# Here is my number for exp/tri3b
%WER 2.12 [ 266 / 12533, 52 ins, 38 del, 176 sub ] exp/tri3b/decode/wer_2


# Below are results of the pdnn recipes
# run-dnn.sh
%WER 1.86 [ 233 / 12533, 24 ins, 57 del, 152 sub ] exp_pdnn/dnn/decode/wer_3

# run-bnf-tandem.sh. Apply mean normalization over BNF, which the default config
%WER 1.76 [ 220 / 12533, 18 ins, 66 del, 136 sub ] exp_pdnn/bnf_tandem/tri4a/decode/wer_14
%WER 1.71 [ 214 / 12533, 16 ins, 62 del, 136 sub ] exp_pdnn/bnf_tandem/tri4a_mmi_b0.1/decode_it1/wer_15
%WER 1.69 [ 212 / 12533, 24 ins, 52 del, 136 sub ] exp_pdnn/bnf_tandem/tri4a_mmi_b0.1/decode_it2/wer_12
%WER 1.64 [ 206 / 12533, 24 ins, 47 del, 135 sub ] exp_pdnn/bnf_tandem/tri4a_mmi_b0.1/decode_it3/wer_12
%WER 1.60 [ 201 / 12533, 23 ins, 44 del, 134 sub ] exp_pdnn/bnf_tandem/tri4a_mmi_b0.1/decode_it4/wer_12

# run-bnf-dnn.sh. Seems that BNF+DNN doesn't work well
%WER 2.03 [ 255 / 12533, 35 ins, 58 del, 162 sub ] exp_pdnn/bnf_dnn/decode/wer_2

# run-fbank.sh
%WER 2.39 [ 299 / 12533, 53 ins, 52 del, 194 sub ] exp_pdnn/dnn_fbank/decode/wer_2

# run-cnn.sh. We get gains from CNN, although not dramatic. But there are various optimizations that could be made:
# 1. the input size is 29x29, but the time dimension might be too large
# 2. the architecture: number of feature maps, pooling size, etc
# 3. learing rate: the starting learning rate 0.08 might be too small, accoring to cnn.fine.log 
%WER 2.18 [ 273 / 12533, 35 ins, 63 del, 175 sub ] exp_pdnn/cnn/decode/wer_2
